<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
<link rel="icon" type="image/png" href="/favicon.png">
<title>AI should be a last resort</title>
<meta name="title" content="AI should be a last resort">
<meta name="description">
<meta name="generator" content="Astro v2.3.0">
<link rel="canonical" href="https://lukealexdavis.co.uk/post/ai-should-be-a-last-resort/">
<link rel="alternate" type="application/rss+xml" title="Luke Alex Davis" href="https://lukealexdavis.co.uk/feed/blog.xml">
<!--link rel="preload" href="/fonts/Karla-Light-subset.woff2" as="font" type="font/woff" crossorigin-->
<link rel="preload" as="style" href="/style/blog.css" type="text/css">
<link rel="stylesheet" type="text/css" href="/style/blog.css">
<script defer data-domain="lukealexdavis.co.uk" src="https://plausible.io/js/plausible.js"></script>
    <script type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://lukealexdavis.co.uk/#website","url":"https://lukealexdavis.co.uk","name":"Luke Davis","publisher":{"@id":"https://lukealexdavis.co.uk/#Person"},"inLanguage":"en-GB"},{"@type":"Article","headline":{"title":"AI should be a last resort"},"datePublished":{"date":"14 May 2023"},"author":{"@type":"Person","name":"Luke Davis","@id":"https://lukealexdavis.co.uk/","url":"https://lukealexdavis.co.uk/"},"publisher":{"@id":"https://lukealexdavis.co.uk/#Person","@type":"Person","name":"Luke Davis"},"name":{"title":"AI should be a last resort"},"@id":"https://lukealexdavis.co.uk/post/ai-should-be-a-last-resort/#richSnippet","inLanguage":"en-GB","mainEntityOfPage":{"@id":"https://lukealexdavis.co.uk/post/ai-should-be-a-last-resort/#webpage"}},{"@type":"Person","image":["https://lukealexdavis.co.uk/images/k5t2wVcf-Luke-Davis-640x600.png","https://lukealexdavis.co.uk/images/lukedavis_jpg.jpg"],"url":"https://lukealexdavis.co.uk","sameAs":["https://uk.linkedin.com/in/lukealexdavis","https://twitter.com/LukeDavisSEO","https://www.instagram.com/lukealexdavis/","https://www.impressiondigital.com/about/our-team/luke-davis/","https://www.searchenginejournal.com/author/luke-davis/","https://www.semrush.com/user/147747585/","https://www.adzooma.com/blog/author/lukedavis/","https://www.patreon.com/lukealexdavis","https://www.wikidata.org/wiki/Q110132394","https://lukealexdavis.contently.com/","https://ko-fi.com/lklxdvs"],"alternateName":["Luke Davis","Luke Alex Davis"],"name":"Luke Davis","jobTitle":"Technical SEO","gender":"Male"}]}</script>
  </head>
  <body>
    <header class="center">
      <nav class="links">
        <ul class="center">
          <li><svg class="logo" xmlns="http://www.w3.org/2000/svg" version="1.1" viewBox="0 0 50 54" width="50px">
  <title>LD wordmark logo</title>
  <desc>A red rounded hexagon with the initials 'ld.' in lower case in the middle.</desc>
  <path fill="#FC1420" stroke="#FC1420" stroke-miterlimit="10" d="M47.085,39.876c2.323,1.354-22.086,15.584-22.086,12.876
      c0,2.708-24.409-11.521-22.085-12.876c-2.324,1.354-2.324-27.105,0-25.752C0.59,12.771,24.999-1.46,24.999,1.248
      c0-2.708,24.409,11.521,22.086,12.876C49.409,12.771,49.409,41.229,47.085,39.876z"></path>
    <path fill="#FFFFFF" d="M16.069,35.021c-0.154,0.53-0.154-17.196,0-16.695c-0.154-0.501,3-3.175,2.973-2.518
      c0.027-0.657,0.027,17.802,0,17.385c0.027,0.418,2.179,1.691,2.081,1.768c0.213,1.134-2.369,2.756-2.378,2.099
      C18.754,37.715,15.915,35.551,16.069,35.021z"></path>
    <path fill="#FFFFFF" d="M22.701,34.112c-0.246,0.369-0.246-7.557,0-7.464c-0.246-0.094,2.91-2.766,2.972-2.517
      c-0.062-0.249-3.691-3.305-3.418-2.878c-0.272-0.427,2.314-2.623,2.437-2.069c-0.122-0.554,6.632,5.176,6.36,5.396
      c0.271-0.221,0.271,8.819,0,8.513c0.271,0.307-4.778,4.569-4.756,4.016C26.272,37.664,22.455,34.481,22.701,34.112z
       M28.087,26.206c0.074-0.196-2.482-2.36-2.408-2.039c-0.074-0.321-0.074,8.589,0,8.393c-0.074,0.196,2.482,2.329,2.408,2.008
      C28.161,34.889,28.161,26.009,28.087,26.206z"></path>
    <path fill="#FFFFFF" d="M34.983,32.576c0.002-0.146,2.905,2.273,2.735,2.278c0.17-0.005-2.764,2.573-2.764,2.426
      c0,0.146-2.936-2.303-2.766-2.306C32.02,34.978,34.985,32.43,34.983,32.576z"></path>
</svg></li>
          <li><a href="/">Home</a></li>
          <li><a href="/about/">About</a></li>
          <li><a href="/posts/">Posts</a></li>
          <li><a href="/release-notes/">Release Notes</a></li>
          <li><a href="/morsels/">Morsels</a></li>
          <li><a href="/wiki/">Wiki</a></li>
          <li><a href="/lists/">Lists</a></li>
          <li><a href="/projects/">Projects</a></li>
          <li><a href="/press/">Press</a></li>
          <li><a href="/support/">Support</a></li>
          <li><a href="/misc/">Misc</a></li>
        </ul>
      </nav>
</header>
    <main class="layout">
  <article class="content">
      <p class="publish-date">14 May 2023</p>
      <h1>AI should be a last resort</h1>
    <section>
      
      <p>Ooh, edgy title! But let me back up my <em>lukewarm</em> take (heh).</p>
<p>We’re told that <a href="/wiki/tech/ai/">AI</a> is simultaneously the present and the future and you’ll be a husk of a human being if you don’t just embrace it and get on the train. Of course, this is absolute nonsense fueled by hypercapitalism but that doesn’t make it any less convincing to “experts” and companies looking for more profit and less outlay (see: <a href="https://techcrunch.com/2023/07/11/tech-industry-layoffs-2023/">tech layoffs</a>).</p>
<p>The problem with this ideology is the implication that whatever you’re doing now is outdated because we said so and regardless of whether it works. Writers aren’t any less qualified or effective than they were in 2019 or before. So why is this burgeoning technology suddenly an answer to a problem that never really existed? Except there <em>is</em> a problem: companies need to recoup the money they lost during the pandemic, and what better way to do that than to <strong>not</strong> pay employees who need pay rises and instead completely replace them with language models they can tinker with at the fraction of the price. If those writers are lucky, they might stay on but have an increased workload (for no extra pay) involving editing <abbr title="large language model">LLM</abbr> outputs rather than writing copy from scratch.</p>
<p>That’s the situation we’re in and will continue to be in while AI is seen as the first port of call to any kind of problem. By taking a step back, you can see the wider picture and realise that there are a multitude of options that come <strong>before</strong> opting for AI. Here are just a few:</p>
<ul>
<li>Ask yourself truthfully if there’s even a problem to solve</li>
<li><a href="https://twitter.com/LukeDavisSEO/status/1657093126471704583">Pay your damn people</a> and let them work if there is</li>
<li>Consider a programmatic approach where applicable (this comes with similar caveats to AI use so don’t fall down a similar trap)</li>
</ul>
<h2 id="i-objectsubject">I object/subject!</h2>
<p>Of course, none of this is to say “don’t even THINK about AI”—I’m not a conservative—but instead think before you leap into something that could cause harm, and arguably is already. Testing stuff is cool and recommended. That’s why I built Oapy with a mirror of OpenAI’s Playground tool. Give this <a href="/post/prompt-engineering/">prompt engineering</a> thing a go, read up on how it really works, and see what sticks and what doesn’t. A great article I read a few weeks ago by Mitchell Hashimoto called <a href="https://mitchellh.com/writing/prompt-engineering-transactional-prompting">Prompt Engineering is for Transactional Prompting
</a> touches on the objective and subjective nature of prompt design and where our LLM use cases fall. I’d found issues with using LLMs for generative purposes despite using all the techniques I’d found from language tech professionals. But Mitchell suggests that these use cases fall in the subjective category and that makes them hard to test for accuracy:</p>
<blockquote>
<p>If we have an input that can produce an objectively correct (or I’d even argue “mostly correct”) output, then prompt engineering can be successfully applied. Examples of problems with objectively correct results: information extraction, classification, limited forms of code generation, etc.</p>
<br>
However, if we have an input that produces something subjectively correct, then prompt engineering is much less useful. The best example of subjective results are creative tasks such as art generation, writing, semantic search.
</blockquote>
<p>Of course, people will—and should—argue that prompt engineering has helped improve creative tasks. But that also, arguably, requires the practioner to have a good understanding of their niche. A very good artist or writer with enough knowledge of what they want and how to achieve it via prompt engineering will get the results they need. It won’t need testing either as they’ll just know based on experience. But someone looking to cut costs and maximise profit will assume you say some magic words and press a button and it’ll just happen because the AI bros said so and they’re never wrong.</p>
<h2 id="cases-where-ai-could-be-better">Cases where AI could be better</h2>
<p>My main argument for AI as a last resort is stronger for those subjective channels of generation. However, with the objective channels such as “information extraction, classification, limited forms of code generation etc.” (ie. most NLP problems) AI can go further up the pecking order. That’s because tools for these tasks have existed for years and have shown varying levels of success. If they work for you, perhaps you don’t need to consider AI at all. But if you need improvements or can’t afford to pay for these tools (a better cost-cutting measure than firing writers I’d say), AI would be a good consideration. That’s because language models in particular are very good at solving NLP problems, with minimal training or supervision. For example, my <a href="/post/introducing-ralts/">RALTS classification tool</a> uses <a href="https://huggingface.co/valhalla/distilbart-mnli-12-9">DistilBart-MNLI</a> with <a href="https://en.wikipedia.org/wiki/Zero-shot_learning">zero-shot learning</a> and the results have been good enough that I’d never want to fine-tune it. Here’s how it classified this blog post:</p>
<p><img src="/images/blog-classification.jpg" alt="A plot showing different classification categories for this post, analysed by a large language model. It suggested that this post is about Technology, Information extraction, Artificial intelligence, Interdisciplinary branches of psychology, Artificial general intelligence, Cognition, Concepts in metaphysics, Branches of science, Cognitive science, and Natural language processing"></p>
<p>Another great article I found a while back titled <a href="https://towardsdatascience.com/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929">“Choosing the right language model for your NLP use case”</a> showed the efficacy of various models (as of September 2022). Some were very good at the aforementioned objective NLP tasks and not so good with content generation, and vice versa. We obviously have ChatGPT and GPT-4 now, as well as a growing number of open source models built of those commercial models, and those gaps are getting smaller by the day. But there isn’t truly a one-size-fits-all model yet and, despite what anyone is telling you, we are no closer to AGI and I don’t think we ever will be so stop talking about it, okay?!</p>
<h2 id="being-conscious-with-my-conscience">Being conscious with my conscience</h2>
<p>I’ve been the main guy at work when it comes to AI. I talk about it a lot, I try to show the best use cases, I’ve even done a webinar on it. But I also show the potential risks and harms of the tech as well because it’s not all interesting and amazing. I worry that the former will outweigh the latter in people’s perceptions and I’ll be the one responsible. That could be unfounded but who am I to say?</p>
<p>I also worry I come off as a hypocrite. I do believe there should be some kind of pause and some kind of regulation but I don’t agree with the <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">open letter</a> that likes of Gary Marcus and Elon Musk signed <em>because</em> Musk was involved, which despite Marcus’s claims should be scrutinised given his <a href="https://www.vanityfair.com/news/2022/04/elon-musk-twitter-terrible-things-hes-said-and-done">practices</a> and <a href="https://www.theverge.com/2023/4/17/23687440/elon-musk-truthgpt-ai-chatgpt">intentions</a>; this isn’t a <a href="https://clickhole.com/heartbreaking-the-worst-person-you-know-just-made-a-gr-1825121606/">The Worst Person You Know Just Made A Great Point</a> moment). I also worry that a regulatory committee will be whitewashed and cause further harm to the women in AI who have been advocating for better ethics for years.</p>
<p>I don’t have any concrete answers to the problems I see in AI but I do know that I want people to be safe. I don’t feel like we’re getting any safer with this tech in its current state and with what we’re told it can, should, is, and will be used for. AI should be the last resort, not the protection of our lives and where we live.</p>
      <div class="tagged-as">Tagged as:</div>
      <p><span class="tags"><a href="/wiki/tech/ai/">AI</a> </span><span class="tags"><a href="/wiki/tech/">tech</a> </span></p>
      <div class="post-pagination">
        <a href=/post/probably-autistic/ class='post-pagination-item post-pagination-prev'>Probably Autistic</a> <a href=/post/post-it-note-directories/ class='post-pagination-item post-pagination-next' >Post-It Note directories</a></div>
      <div>
  <h3>Random posts you might be interested in:</h3>
  <ul>
  <li class="tags"><a href="https://lukealexdavis.co.uk/post/eight/">Eight</a></li><li class="tags"><a href="https://lukealexdavis.co.uk/post/morsels/morsel-8/">Morsel #8: Bandcamp Album Length Calculator</a></li><li class="tags"><a href="https://lukealexdavis.co.uk/post/blackness/">What is blackness?</a></li><li class="tags"><a href="https://lukealexdavis.co.uk/post/my-album-of-the-year-list-2014/">My Album of the Year List: 2014</a></li>
</ul>
</div>
    
    </section>
  </article>
</main>
    <footer>
    <p>Built with HTML, CSS, and <a href="https://astro.build/">Astro</a> which has made this super fun. <a href="https://twitter.com/t3dotgg/status/1481344430921502720">Special tweet added for good luck.</a></p>
    <p>[ <a href="/html-sitemap/">HTML Sitemap</a> ]</p>
    <p>Member of the <a href="https://250kb.club/">250KB Club</a>, <a href="https://512kb.club/">512KB Club</a> and <a href="https://1mb.club/">1MB Club</a></p>
    <p><a href="https://hotlinewebring.club/lukealexdavis/previous">&#8604;</a> Member of the <a href="https://hotlinewebring.club/">Hotline Webring</a> <a href="https://hotlinewebring.club/lukealexdavis/next">&rarrw;</a></p>
    <p>And I'm not only a member of the Hair Club for Men&mdash;I'm also the president.</p>
    <p>Listed @ <a href="https://linklane.net" target="_blank">LinkLane.Net</a></p>
    <p>- - - - -</p>
    <a href="https://github.com/uxai/non-profit-bloggers/">
        <svg width="118" height="20" viewBox="0 0 118 20" fill="none" xmlns="http://www.w3.org/2000/svg">
            <title>Non-Profit Blogger</title>
            <desc>A lilac badge with the words "Non-Profit Bloggers" in the middle.</desc>
            <rect x="0.5" y="0.5" width="117" height="19" rx="2.5" fill="url(#paint0_linear_55_5)"></rect>
            <rect x="0.5" y="0.5" width="117" height="19" rx="2.5" fill="url(#paint1_linear_55_5)"></rect>
            <path d="M7.91662 6.25002C7.27683 6.25002 6.63396 6.45833 6.14579 6.95833C5.1695 7.95833 5.19629 9.49999 6.14579 10.5L9.99996 14.5833L13.8541 10.5C14.8037 9.49999 14.8304 7.95833 13.8541 6.95833C12.8779 5.99999 11.2887 5.99999 10.3125 6.95833L9.99996 7.29168L9.68746 6.95833C9.19912 6.45833 8.55642 6.25002 7.91662 6.25002Z" fill="#492E76"></path>
            <path d="M19.6875 6.8125H21.0625V8.875H21.75V9.5625H22.4375V6.8125H23.8125V13H22.4375V10.9375H21.75V10.25H21.0625V13H19.6875V6.8125ZM25.875 6.8125H28.625V7.5H29.3125V12.3125H28.625V13H25.875V12.3125H25.1875V7.5H25.875V6.8125ZM27.9375 7.5H26.5625V12.3125H27.9375V7.5ZM30.6875 6.8125H32.0625V8.875H32.75V9.5625H33.4375V6.8125H34.8125V13H33.4375V10.9375H32.75V10.25H32.0625V13H30.6875V6.8125ZM36.1875 9.5625H38.9375V10.25H36.1875V9.5625ZM40.3125 6.8125H43.75V7.5H44.4375V9.5625H43.75V10.25H41.6875V13H40.3125V6.8125ZM43.0625 7.5H41.6875V9.5625H43.0625V7.5ZM48.5625 7.5H47.1875V9.5625H48.5625V7.5ZM45.8125 6.8125H49.25V7.5H49.9375V9.5625H49.25V10.25H48.5625V10.9375H49.25V11.625H49.9375V13H48.5625V11.625H47.875V10.9375H47.1875V13H45.8125V6.8125ZM52 6.8125H54.75V7.5H55.4375V12.3125H54.75V13H52V12.3125H51.3125V7.5H52V6.8125ZM54.0625 7.5H52.6875V12.3125H54.0625V7.5ZM56.8125 6.8125H60.9375V7.5H58.1875V9.5625H59.5625V10.25H58.1875V13H56.8125V6.8125ZM63 6.8125H64.375V13H63V6.8125ZM66.4375 6.8125H70.5625V7.5H69.1875V13H67.8125V7.5H66.4375V6.8125ZM74.6875 6.8125H78.125V7.5H78.8125V9.5625H78.125V10.25H78.8125V12.3125H78.125V13H74.6875V6.8125ZM77.4375 7.5H76.0625V9.5625H77.4375V7.5ZM77.4375 10.25H76.0625V12.3125H77.4375V10.25ZM80.1875 6.8125H81.5625V12.3125H84.3125V13H80.1875V6.8125ZM86.375 6.8125H89.125V7.5H89.8125V12.3125H89.125V13H86.375V12.3125H85.6875V7.5H86.375V6.8125ZM88.4375 7.5H87.0625V12.3125H88.4375V7.5ZM91.875 6.8125H94.625V7.5H95.3125V8.1875H93.9375V7.5H92.5625V12.3125H93.9375V10.25H93.25V9.5625H95.3125V13H91.875V12.3125H91.1875V7.5H91.875V6.8125ZM97.375 6.8125H100.125V7.5H100.812V8.1875H99.4375V7.5H98.0625V12.3125H99.4375V10.25H98.75V9.5625H100.812V13H97.375V12.3125H96.6875V7.5H97.375V6.8125ZM102.188 6.8125H106.312V7.5H103.562V9.5625H104.938V10.25H103.562V12.3125H106.312V13H102.188V6.8125ZM110.438 7.5H109.062V9.5625H110.438V7.5ZM107.688 6.8125H111.125V7.5H111.812V9.5625H111.125V10.25H110.438V10.9375H111.125V11.625H111.812V13H110.438V11.625H109.75V10.9375H109.062V13H107.688V6.8125Z" fill="#492E76"></path>
            <rect x="0.5" y="0.5" width="117" height="19" rx="2.5" stroke="#9D9CD3"></rect>
            <defs>
            <linearGradient id="paint0_linear_55_5" x1="1" y1="1" x2="117.69" y2="10.6591" gradientUnits="userSpaceOnUse">
                <stop offset="0.1" stop-color="#BEBDFF"></stop>
                <stop offset="1" stop-color="#D1ADFF"></stop>
            </linearGradient>
            <linearGradient id="paint1_linear_55_5" x1="59" y1="1" x2="59" y2="1.5" gradientUnits="userSpaceOnUse">
                <stop offset="0.9999" stop-color="white"></stop>
                <stop offset="1" stop-color="white" stop-opacity="0"></stop>
            </linearGradient>
            </defs>
        </svg>
    </a>
    <p>- - - - -</p>
    <p>Copyright 2023 © Luke Davis</p>
</footer>
  </body></html>